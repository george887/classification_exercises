{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "\n",
    "|               | actual cat | actual dog \n",
    "|:------------  |-----------:|-----------:|\n",
    "| predicted cat |         34 |          7 |\n",
    "| predicted dog |         13 |         46 |\n",
    "\n",
    "\n",
    "- In the context of this problem, what is a false positive?\n",
    "- In the context of this problem, what is a false negative?\n",
    "- How would you describe this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context of this problem, what is a false positive?\n",
    "# False alarm. Predicted cat and it was an actual dog. Value == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context of this problem, what is a false negative?\n",
    "# Miss. Predicted dog and was actually a cat. Value == 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 80.0% accurate.\n"
     ]
    }
   ],
   "source": [
    "# How would you describe this model?\n",
    "TP = 34\n",
    "TN = 46\n",
    "FP = 7\n",
    "FN = 13\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "print(f\"The model is {accuracy}% accurate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which evaluation metric would be appropriate here?\n",
    "# The team wants to investigate the defects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_duck = pd.read_csv('untidy_data/c3.csv')\n",
    "r_duck.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    184\n",
       "Defect        16\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts to see how many defects and non defects there are\n",
    "r_duck.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3   baseline\n",
       "0  No Defect  No Defect  Defect  No Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect  No Defect\n",
       "2  No Defect  No Defect  Defect  No Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect  No Defect\n",
       "4  No Defect  No Defect  Defect  No Defect  No Defect"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the baseline as No defect positive due to having the most observations\n",
    "r_duck[\"baseline\"] = r_duck.actual.value_counts().index[0]\n",
    "r_duck.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model1                      \n",
       "Defect          8          2\n",
       "No Defect       8        182"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline shows there are more no defects than defects.\n",
    "# Basline is to predict no defects, so we should use specificity to calculate TN out of\n",
    "# all Actual Negatives since the baseline is predicting positives\n",
    "actual_v_m1 = pd.crosstab(r_duck.model1, r_duck.actual)\n",
    "actual_v_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 95.0% accurate.\n",
      "The model is 95.0% accurate with the DF calculation.\n",
      "The models specificity is 50.0%.\n"
     ]
    }
   ],
   "source": [
    "# Model_1 accuracy and specificity\n",
    "TP = 182\n",
    "TN = 8\n",
    "FP = 8\n",
    "FN = 2\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "accuracy\n",
    "accuracy_2 = (r_duck.actual == r_duck.model1).mean() * 100\n",
    "print(f\"The model is {accuracy}% accurate.\")\n",
    "print(f\"The model is {round(accuracy_2,2)}% accurate with the DF calculation.\")\n",
    "# % of predicting TN out of all actual Negatives\n",
    "specificity = TN/(TN + FP) * 100\n",
    "print(f\"The models specificity is {round(specificity,2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model2                      \n",
       "Defect          9         81\n",
       "No Defect       7        103"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_v_m2 = pd.crosstab(r_duck.model2, r_duck.actual)\n",
    "actual_v_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 56.0% accurate.\n",
      "The model is 56.0% accurate with the DF calculation.\n",
      "The models specificity is 56.25%.\n"
     ]
    }
   ],
   "source": [
    "# Model_2 accuracy and specificty\n",
    "TP = 103\n",
    "TN = 9\n",
    "FP = 7\n",
    "FN = 81\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "accuracy_2 = (r_duck.actual == r_duck.model2).mean() * 100\n",
    "print(f\"The model is {round(accuracy,2)}% accurate.\")\n",
    "print(f\"The model is {round(accuracy_2,2)}% accurate with the DF calculation.\")\n",
    "# % of predicting TN out of all actual Negatives\n",
    "specificity = TN/(TN + FP) * 100\n",
    "print(f\"The models specificity is {round(specificity,2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>13</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model3                      \n",
       "Defect         13         86\n",
       "No Defect       3         98"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_v_m3 = pd.crosstab(r_duck.model3, r_duck.actual)\n",
    "actual_v_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 55.5% accurate.\n",
      "The model is 55.5% accurate with the DF calculation.\n",
      "The models specificity is 81.25%.\n"
     ]
    }
   ],
   "source": [
    "# Model_3 accuracy and specificity\n",
    "TP = 98\n",
    "TN = 13\n",
    "FP = 3\n",
    "FN = 86\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "accuracy\n",
    "accuracy_2 = (r_duck.actual == r_duck.model3).mean() * 100\n",
    "print(f\"The model is {round(accuracy,2)}% accurate.\")\n",
    "print(f\"The model is {round(accuracy_2,2)}% accurate with the DF calculation.\")\n",
    "# % of predicting TN out of all actual Negatives\n",
    "specificity = TN/(TN + FP) * 100\n",
    "print(f\"The models specificity is {round(specificity,2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Defect       0.50      0.80      0.62        10\n",
      "   No Defect       0.99      0.96      0.97       190\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.74      0.88      0.79       200\n",
      "weighted avg       0.96      0.95      0.96       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Defect       0.56      0.10      0.17        90\n",
      "   No Defect       0.56      0.94      0.70       110\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.56      0.52      0.44       200\n",
      "weighted avg       0.56      0.56      0.46       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Defect       0.81      0.13      0.23        99\n",
      "   No Defect       0.53      0.97      0.69       101\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.67      0.55      0.46       200\n",
      "weighted avg       0.67      0.56      0.46       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n",
    "print(classification_report(r_duck.model1, r_duck.actual))\n",
    "print(classification_report(r_duck.model2, r_duck.actual))\n",
    "print(classification_report(r_duck.model3, r_duck.actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 shows the highest specificity that is predicting the TN out of all actual negatives at 81.25% since the baseline is predicting positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models recall is 98.91%.\n"
     ]
    }
   ],
   "source": [
    "# They want us to predict defects but don't want to accidently give out a vacation if \n",
    "# there is no defect. We would pick the recall metric or true positive predictions value \n",
    "# Want to avoid FN\n",
    "# model_1\n",
    "TP = 182\n",
    "TN = 8\n",
    "FP = 8\n",
    "FN = 2\n",
    "\n",
    "recall = TP/(TP + FN) * 100\n",
    "print(f\"The models recall is {round(recall,2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models recall is 98.91% using the DF calculation.\n",
      "The baseline recall is 100.0% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# the boolean mask here is model1 == positive. Using DF to calculate recall\n",
    "positive = \"No Defect\"\n",
    "subset = r_duck[r_duck.actual == positive]\n",
    "model1_recall = (subset.model1 == subset.actual).mean() * 100\n",
    "#subset = r_duck[r_duck.baseline == positive]\n",
    "baseline_recall = (subset.baseline == subset.actual).mean() * 100\n",
    "print(f\"The models recall is {round(model1_recall,2)}% using the DF calculation.\")\n",
    "print(f\"The baseline recall is {baseline_recall}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models recall is 55.98%.\n"
     ]
    }
   ],
   "source": [
    "# model_2\n",
    "TP = 103\n",
    "TN = 9\n",
    "FP = 7\n",
    "FN = 81\n",
    "\n",
    "recall = TP/(TP + FN) * 100\n",
    "print(f\"The models recall is {round(recall,2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models recall is 55.98% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# the boolean mask here is model1 == positive. Using DF to calculate recall\n",
    "model2_recall = (subset.model2 == subset.actual).mean() * 100\n",
    "print(f\"The models recall is {round(model2_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models recall is 53.26%.\n"
     ]
    }
   ],
   "source": [
    "# model_3\n",
    "TP = 98\n",
    "TN = 13\n",
    "FP = 3\n",
    "FN = 86\n",
    "recall = TP/(TP + FN) * 100\n",
    "print(f\"The models recall is {round(recall,2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models recall is 53.26% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# the boolean mask here is model1 == positive. Using DF to calculate recall\n",
    "model3_recall = (subset.model3 == subset.actual).mean() * 100\n",
    "print(f\"The models recall is {round(model3_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the 3 models. Model_1 show the highest recall or the true positive rate AKA sensitivity value at 98.91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "- At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "- Several models have already been developed with the data, and you can find their results here.\n",
    "\n",
    "- Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws = pd.read_csv('untidy_data/gives_you_paws.csv')\n",
    "paws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    3254\n",
       "cat    1746\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show more dogs than cats. Baseline is that they will give you a dog pic\n",
    "paws.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.08"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = 3254/(3254+1746) * 100\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      dog\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "4    cat    cat    cat    dog    dog      dog"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on the value counts, predicting dog is the baseline\n",
    "paws['baseline'] = paws.actual.value_counts().index[0]\n",
    "paws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.08"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_accuracy = (paws.baseline == paws.actual).mean() *100\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1423</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>323</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model1            \n",
       "cat     1423   640\n",
       "dog      323  2614"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model_1\n",
    "a_v_m1 = pd.crosstab(paws.model1, paws.actual)\n",
    "a_v_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 80.74% accurate.\n",
      "The model is 80.74% accurate using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# In terms of accuracy, how do the various models compare to the baseline model? Are any\n",
    "# of the models better than the baseline?\n",
    "# Model1\n",
    "TP = 2614\n",
    "TN = 1423\n",
    "FP = 323\n",
    "FN = 640\n",
    "\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "accuracy\n",
    "model1_accuracy = (paws.model1 == paws.actual).mean()*100\n",
    "print(f\"The model is {round(accuracy,2)}% accurate.\")\n",
    "print(f\"The model is {model1_accuracy}% accurate using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1555</td>\n",
       "      <td>1657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>191</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model2            \n",
       "cat     1555  1657\n",
       "dog      191  1597"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model2\n",
    "a_v_m2 = pd.crosstab(paws.model2, paws.actual)\n",
    "a_v_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 63.04% accurate.\n",
      "The model is 63.04% accurate using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model2\n",
    "TP = 1597\n",
    "TN = 1555\n",
    "FP = 191\n",
    "FN = 1657\n",
    "\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "accuracy\n",
    "model2_accuracy = (paws.model2 == paws.actual).mean() *100\n",
    "print(f\"The model is {round(accuracy,2)}% accurate.\")\n",
    "print(f\"The model is {round(model2_accuracy,2)}% accurate using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>893</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>853</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual  cat   dog\n",
       "model3           \n",
       "cat     893  1599\n",
       "dog     853  1655"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "a_v_m3 = pd.crosstab(paws.model3, paws.actual)\n",
    "a_v_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 50.96% accurate.\n",
      "The model is 50.96% accurate using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model3\n",
    "TP = 1655\n",
    "TN = 893\n",
    "FP = 853\n",
    "FN = 1599\n",
    "\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "accuracy\n",
    "model3_accuracy = (paws.model3 == paws.actual).mean() *100\n",
    "print(f\"The model is {round(accuracy,2)}% accurate.\")\n",
    "print(f\"The model is {round(model3_accuracy,2)}% accurate using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>603</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1143</td>\n",
       "      <td>3110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model4            \n",
       "cat      603   144\n",
       "dog     1143  3110"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model4\n",
    "a_v_m4 = pd.crosstab(paws.model4, paws.actual)\n",
    "a_v_m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 74.26% accurate.\n",
      "The model is 74.26% accurate using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model4\n",
    "TP = 3110\n",
    "TN = 603\n",
    "FP = 1143\n",
    "FN = 144\n",
    "\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN) * 100\n",
    "accuracy\n",
    "model4_accuracy = (paws.model4 == paws.actual).mean() * 100\n",
    "print(f\"The model is {round(accuracy,2)}% accurate.\")\n",
    "print(f\"The model is {round(model4_accuracy,2)}% accurate using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the accuracy calculation, model 1 is the most accurate with 80.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to use recall for phase 1. We're optimizing for true positive cases / (all positve cases)\n",
    "# Phase 2 use precesion % of positive predictions that are correct. trying to minimize false \n",
    "# positives (the model saying dog, but we have a cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual   cat   dog\n",
      "model1            \n",
      "cat     1423   640\n",
      "dog      323  2614\n",
      "The model has a recall rate of 80.33%.\n",
      "The model has a recall rate of 80.33% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "#paws = pd.read_csv('gives_you_paws.csv')\n",
    "act_vs_model1 = pd.crosstab(paws.model1, paws.actual)\n",
    "print(act_vs_model1)\n",
    "# Model1\n",
    "TP = 2614\n",
    "TN = 1423\n",
    "FP = 323\n",
    "FN = 640\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"dog\"]\n",
    "model1_recall = (subset.model1 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "print(f\"The model has a recall rate of {round(model1_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a recall rate of 49.08%.\n",
      "The model has a recall rate of 49.08% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model2\n",
    "TP = 1597\n",
    "TN = 1555\n",
    "FP = 191\n",
    "FN = 1657\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"dog\"]\n",
    "model2_recall = (subset.model2 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "print(f\"The model has a recall rate of {round(model2_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a recall rate of 50.86%.\n",
      "The model has a recall rate of 50.86% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model3\n",
    "TP = 1655\n",
    "TN = 893\n",
    "FP = 853\n",
    "FN = 1599\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"dog\"]\n",
    "model3_recall = (subset.model3 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "print(f\"The model has a recall rate of {round(model3_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a recall rate of 95.57%.\n",
      "The model has a recall rate of 95.57% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model4\n",
    "TP = 3110\n",
    "TN = 603\n",
    "FP = 1143\n",
    "FN = 144\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"dog\"]\n",
    "model4_recall = (subset.model4 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "print(f\"The model has a recall rate of {round(model4_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 has the best recall rate of 95.57%. We accounted for all predicted positives\n",
    "# Phase 2 use precesion % of positive predictions that are correct. Want to reduce\n",
    "# the false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a precision rate of 89.0%.\n",
      "The model has a precision rate of 89.0% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model1\n",
    "TP = 2614\n",
    "TN = 1423\n",
    "FP = 323\n",
    "FN = 640\n",
    "\n",
    "precision = TP/(TP + FP) * 100\n",
    "subset = paws[paws.model1 == \"dog\"]\n",
    "model1_precision = (subset.model1 == subset.actual).mean() * 100\n",
    "print(f\"The model has a precision rate of {round(precision,2)}%.\")\n",
    "print(f\"The model has a precision rate of {round(model1_precision,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a precision rate of 89.32%.\n",
      "The model has a precision rate of 89.32% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model2\n",
    "TP = 1597\n",
    "TN = 1555\n",
    "FP = 191\n",
    "FN = 1657\n",
    "\n",
    "precision = TP/(TP + FP) * 100\n",
    "subset = paws[paws.model2 == \"dog\"]\n",
    "model2_precision = (subset.model2 == subset.actual).mean() * 100\n",
    "print(f\"The model has a precision rate of {round(precision,2)}%.\")\n",
    "print(f\"The model has a precision rate of {round(model2_precision,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a precision rate of 65.99%.\n",
      "The model has a precision rate of 65.99% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model3\n",
    "TP = 1655\n",
    "TN = 893\n",
    "FP = 853\n",
    "FN = 1599\n",
    "\n",
    "precision = TP/(TP + FP) * 100\n",
    "subset = paws[paws.model3 == \"dog\"]\n",
    "model3_precision = (subset.model3 == subset.actual).mean() * 100\n",
    "print(f\"The model has a precision rate of {round(precision,2)}%.\")\n",
    "print(f\"The model has a precision rate of {round(model3_precision,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a precision rate of 73.12%.\n",
      "The model has a precision rate of 73.12% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model4\n",
    "TP = 3110\n",
    "TN = 603\n",
    "FP = 1143\n",
    "FN = 144\n",
    "\n",
    "precision = TP/(TP + FP) * 100\n",
    "subset = paws[paws.model4 == \"dog\"]\n",
    "model4_precision = (subset.model4 == subset.actual).mean() * 100\n",
    "print(f\"The model has a precision rate of {round(precision,2)}%.\")\n",
    "print(f\"The model has a precision rate of {round(model4_precision,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 shows the best recall rate of 89.32%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase I should be recall to optimize for # of true positive cases out of all actual positive cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase II should be precision to minimize False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1423</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>323</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model1            \n",
       "cat     1423   640\n",
       "dog      323  2614"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_vs_model1 = pd.crosstab(paws.model1, paws.actual)\n",
    "act_vs_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a recall rate of 81.5%.\n",
      "The model has a recall rate of 81.5% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model1\n",
    "# Have to adjust our confusion matrix to accomodate for cats\n",
    "# Model1\n",
    "TP = 1423\n",
    "TN = 2614\n",
    "FP = 640\n",
    "FN = 323\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"cat\"]\n",
    "model1_recall = (subset.model1 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "print(f\"The model has a recall rate of {round(model1_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1555</td>\n",
       "      <td>1657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>191</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model2            \n",
       "cat     1555  1657\n",
       "dog      191  1597"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_v_m2 = pd.crosstab(paws.model2, paws.actual)\n",
    "a_v_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a recall rate of 89.06%.\n",
      "The model has a recall rate of 89.06% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model2\n",
    "TP = 1555\n",
    "TN = 1597\n",
    "FP = 1657\n",
    "FN = 191\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"cat\"]\n",
    "model2_recall = (subset.model2 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "print(f\"The model has a recall rate of {round(model2_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>893</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>853</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual  cat   dog\n",
       "model3           \n",
       "cat     893  1599\n",
       "dog     853  1655"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_v_m3 = pd.crosstab(paws.model3, paws.actual)\n",
    "a_v_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a recall rate of 51.15%.\n",
      "The model has a recall rate of 51.15% using the DF calculation.\n"
     ]
    }
   ],
   "source": [
    "# Model3\n",
    "TP = 893\n",
    "TN = 1655\n",
    "FP = 1599\n",
    "FN = 853\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"cat\"]\n",
    "model3_recall = (subset.model3 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "print(f\"The model has a recall rate of {round(model3_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>603</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1143</td>\n",
       "      <td>3110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model4            \n",
       "cat      603   144\n",
       "dog     1143  3110"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_v_m4 = pd.crosstab(paws.model4, paws.actual)\n",
    "a_v_m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a recall rate of 34.54%.\n"
     ]
    }
   ],
   "source": [
    "# Model4\n",
    "TP = 603\n",
    "TN = 3110\n",
    "FP = 144\n",
    "FN = 1143\n",
    "\n",
    "recall = (TP)/(TP + FN) * 100\n",
    "subset = paws[paws.actual == \"cat\"]\n",
    "model4_recall = (subset.model4 == subset.actual).mean() * 100\n",
    "print(f\"The model has a recall rate of {round(recall,2)}%.\")\n",
    "#print(f\"The model has a recall rate of {round(model4_recall,2)}% using the DF calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would want to use model 2 with a recall rate of 89.06%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add model_evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 7e702ca] Updates to evaluations\r\n",
      " 1 file changed, 378 insertions(+), 100 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Updates to evaluations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 1.19 KiB | 611.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/george887/classification_exercises.git\n",
      "   be3cbb6..7e702ca  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
